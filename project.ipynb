{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\lucas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\lucas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user nltk\n",
    "\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreive document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(documents): 1400\n",
      "('471', None, None, None, None)\n",
      "('995', None, None, None, None)\n",
      "len(documents): 1398\n",
      "[('1', 'experimental investigation of the aerodynamics of a\\nwing in a slipstream .', 'brenckman,m.', 'j. ae. scs. 25, 1958, 324.', 'experimental investigation of the aerodynamics of a\\nwing in a slipstream .\\n  an experimental study of a wing in a propeller slipstream was\\nmade in order to determine the spanwise distribution of the lift\\nincrease due to slipstream at different angles of attack of the wing\\nand at different free stream to slipstream velocity ratios .  the\\nresults were intended in part as an evaluation basis for different\\ntheoretical treatments of this problem .\\n  the comparative span loading curves, together with\\nsupporting evidence, showed that a substantial part of the lift increment\\nproduced by the slipstream was due to a /destalling/ or\\nboundary-layer-control effect .  the integrated remaining lift\\nincrement, after subtracting this destalling lift, was found to agree\\nwell with a potential flow theory .\\n  an empirical evaluation of the destalling effects was made for\\nthe specific configuration of the experiment .'), ('2', 'simple shear flow past a flat plate in an incompressible fluid of small\\nviscosity .', 'ting-yili', 'department of aeronautical engineering, rensselaer polytechnic\\ninstitute\\ntroy, n.y.', \"simple shear flow past a flat plate in an incompressible fluid of small\\nviscosity .\\nin the study of high-speed viscous flow past a two-dimensional body it\\nis usually necessary to consider a curved shock wave emitting from the\\nnose or leading edge of the body .  consequently, there exists an\\ninviscid rotational flow region between the shock wave and the boundary\\nlayer .  such a situation arises, for instance, in the study of the\\nhypersonic viscous flow past a flat plate .  the situation is somewhat\\ndifferent from prandtl's classical boundary-layer problem . in prandtl's\\noriginal problem the inviscid free stream outside the boundary layer is\\nirrotational while in a hypersonic boundary-layer problem the inviscid\\nfree stream must be considered as rotational .  the possible effects of\\nvorticity have been recently discussed by ferri and libby .  in the\\npresent paper, the simple shear flow past a flat plate in a fluid of small\\nviscosity is investigated .  it can be shown that this problem can again\\nbe treated by the boundary-layer approximation, the only novel feature\\nbeing that the free stream has a constant vorticity .  the discussion\\nhere is restricted to two-dimensional incompressible steady flow .\"), ('3', 'the boundary layer in simple shear flow past a flat plate .', 'm. b. glauert', 'department of mathematics, university of manchester, manchester,\\nengland', 'the boundary layer in simple shear flow past a flat plate .\\nthe boundary-layer equations are presented for steady\\nincompressible flow with no pressure gradient .'), ('4', 'approximate solutions of the incompressible laminar\\nboundary layer equations for a plate in shear flow .', 'yen,k.t.', 'j. ae. scs. 22, 1955, 728.', 'approximate solutions of the incompressible laminar\\nboundary layer equations for a plate in shear flow .\\n  the two-dimensional steady boundary-layer\\nproblem for a flat plate in a\\nshear flow of incompressible fluid is considered .\\nsolutions for the boundary-\\nlayer thickness, skin friction, and the velocity\\ndistribution in the boundary\\nlayer are obtained by the karman-pohlhausen\\ntechnique .  comparison with\\nthe boundary layer of a uniform flow has also\\nbeen made to show the effect of\\nvorticity .'), ('5', 'one-dimensional transient heat conduction into a double-layer\\nslab subjected to a linear heat input for a small time\\ninternal .', 'wasserman,b.', 'j. ae. scs. 24, 1957, 924.', 'one-dimensional transient heat conduction into a double-layer\\nslab subjected to a linear heat input for a small time\\ninternal .\\n  analytic solutions are presented for the transient heat\\nconduction in composite slabs exposed at one surface to a\\ntriangular heat rate .  this type of heating rate may occur, for\\nexample, during aerodynamic heating .')]\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    documents = []\n",
    "    for doc in root.findall('.//doc'):  # Look for all <doc> elements\n",
    "        docno = doc.find('.//docno').text\n",
    "        title = doc.find('.//title').text\n",
    "        author = doc.find('.//author').text\n",
    "        bib = doc.find('.//bib').text\n",
    "        text = doc.find('.//text').text\n",
    "        documents.append((docno, title, author, bib, text))\n",
    "    return documents\n",
    "\n",
    "\n",
    "documents = parse_xml(\"cran.all.1400.xml\")\n",
    "print(\"len(documents):\", len(documents))\n",
    "for row in documents:\n",
    "    if row[4] is None:\n",
    "        print(row)\n",
    "    \n",
    "documents = [row for row in documents if row[4] is not None]\n",
    "\n",
    "print(\"len(documents):\", len(documents))\n",
    "print(documents[:5])  # Display first 5 documents for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'run', 'fast', 'look', 'like', 'fli', 'fuck', 'sake']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')  # Download WordNet data\n",
    "nltk.download('stopwords')  # Download stopwords list\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove newline characters and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace all kinds of whitespace with a single space\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Keep only alphanumeric and whitespace characters\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    filtered_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    return filtered_tokens\n",
    "\n",
    "example_text = \"The quick brown fox is running fast but it looks like he is flying for fuck sake!\"\n",
    "processed_text = preprocess_text(example_text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Inverted Index Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment': [('1', 3), ('11', 1), ('12', 1), ('16', 1), ('17', 1), ('19', 1), ('25', 1), ('29', 1), ('30', 2), ('35', 1), ('37', 1), ('41', 1), ('43', 1), ('47', 1), ('52', 2), ('53', 1), ('58', 1), ('69', 1), ('70', 1), ('74', 2), ('78', 2), ('84', 3), ('99', 2), ('101', 1), ('103', 1), ('112', 1), ('115', 1), ('121', 1), ('123', 3), ('131', 1), ('137', 1), ('140', 1), ('142', 1), ('154', 1), ('156', 1), ('167', 1), ('168', 1), ('170', 1), ('171', 2), ('173', 2), ('176', 1), ('179', 2), ('183', 1), ('184', 1), ('186', 3), ('187', 1), ('188', 1), ('189', 2), ('191', 1), ('195', 3), ('197', 2), ('202', 1), ('203', 1), ('206', 2), ('207', 2), ('212', 1), ('216', 1), ('220', 1), ('222', 1), ('225', 2), ('227', 1), ('230', 1), ('234', 4), ('245', 1), ('251', 1), ('256', 3), ('257', 1), ('262', 1), ('271', 3), ('273', 1), ('277', 1), ('282', 1), ('283', 1), ('286', 1), ('287', 1), ('289', 1), ('294', 1), ('295', 1), ('304', 1), ('307', 1), ('329', 2), ('330', 2), ('334', 2), ('338', 1), ('339', 2), ('344', 3), ('345', 1), ('346', 3), ('347', 1), ('354', 1), ('360', 1), ('369', 1), ('370', 1), ('372', 3), ('377', 1), ('397', 1), ('409', 1), ('411', 2), ('413', 2), ('418', 1), ('420', 2), ('421', 1), ('423', 2), ('427', 1), ('433', 1), ('435', 1), ('439', 1), ('441', 2), ('442', 3), ('443', 1), ('453', 1), ('455', 2), ('462', 1), ('464', 1), ('467', 1), ('484', 3), ('494', 2), ('496', 1), ('497', 2), ('498', 1), ('501', 1), ('503', 1), ('504', 1), ('505', 1), ('511', 1), ('517', 1), ('518', 2), ('519', 1), ('520', 2), ('522', 3), ('536', 1), ('540', 1), ('544', 3), ('549', 2), ('552', 2), ('553', 1), ('558', 2), ('563', 1), ('567', 1), ('569', 2), ('572', 4), ('576', 1), ('588', 1), ('595', 1), ('600', 1), ('606', 1), ('610', 1), ('632', 1), ('634', 1), ('635', 1), ('636', 1), ('644', 1), ('645', 1), ('649', 1), ('658', 1), ('662', 2), ('663', 2), ('666', 2), ('670', 1), ('675', 1), ('678', 1), ('679', 1), ('685', 3), ('688', 4), ('689', 2), ('694', 1), ('704', 2), ('712', 1), ('713', 1), ('717', 1), ('720', 1), ('725', 1), ('728', 1), ('729', 1), ('739', 1), ('740', 1), ('743', 1), ('753', 1), ('760', 3), ('764', 1), ('766', 4), ('767', 3), ('772', 3), ('781', 2), ('790', 1), ('801', 3), ('802', 1), ('806', 1), ('816', 2), ('820', 1), ('823', 2), ('825', 1), ('827', 2), ('830', 1), ('836', 5), ('844', 1), ('845', 2), ('846', 1), ('847', 2), ('856', 4), ('857', 3), ('858', 3), ('863', 2), ('866', 2), ('867', 1), ('869', 1), ('878', 2), ('881', 1), ('887', 1), ('891', 2), ('907', 1), ('911', 2), ('912', 2), ('923', 1), ('924', 1), ('927', 3), ('928', 3), ('932', 2), ('935', 2), ('946', 2), ('950', 4), ('951', 1), ('954', 2), ('955', 1), ('959', 1), ('961', 1), ('964', 1), ('965', 1), ('973', 1), ('974', 1), ('984', 2), ('986', 4), ('996', 1), ('997', 4), ('999', 1), ('1006', 1), ('1008', 1), ('1016', 1), ('1019', 3), ('1028', 1), ('1039', 3), ('1040', 1), ('1045', 1), ('1046', 1), ('1049', 2), ('1051', 1), ('1062', 3), ('1066', 4), ('1069', 1), ('1070', 1), ('1074', 3), ('1075', 3), ('1076', 1), ('1078', 1), ('1080', 1), ('1081', 1), ('1082', 1), ('1083', 1), ('1092', 1), ('1097', 3), ('1098', 2), ('1110', 1), ('1112', 1), ('1118', 2), ('1122', 2), ('1125', 1), ('1127', 1), ('1145', 1), ('1146', 1), ('1151', 1), ('1153', 1), ('1155', 2), ('1156', 3), ('1158', 1), ('1159', 2), ('1160', 2), ('1161', 2), ('1167', 2), ('1171', 1), ('1172', 1), ('1177', 1), ('1185', 1), ('1186', 1), ('1187', 1), ('1192', 1), ('1195', 1), ('1196', 2), ('1198', 1), ('1199', 1), ('1204', 3), ('1205', 1), ('1209', 3), ('1212', 1), ('1213', 2), ('1214', 3), ('1216', 1), ('1218', 2), ('1220', 1), ('1222', 1), ('1225', 3), ('1227', 1), ('1228', 1), ('1230', 1), ('1231', 1), ('1234', 1), ('1237', 1), ('1261', 1), ('1262', 1), ('1263', 2), ('1264', 2), ('1268', 1), ('1269', 2), ('1277', 2), ('1290', 1), ('1298', 1), ('1302', 2), ('1310', 1), ('1314', 2), ('1317', 1), ('1319', 1), ('1324', 1), ('1337', 3), ('1338', 2), ('1339', 1), ('1341', 1), ('1352', 2), ('1363', 2), ('1364', 2), ('1369', 1), ('1372', 1), ('1374', 1), ('1378', 1), ('1384', 1), ('1390', 1), ('1392', 1), ('1396', 1), ('1397', 1)], 'investig': [('1', 2), ('2', 1), ('7', 1), ('8', 1), ('9', 3), ('19', 2), ('29', 1), ('30', 2), ('34', 2), ('37', 3), ('40', 2), ('42', 1), ('44', 2), ('45', 2), ('50', 3), ('51', 1), ('56', 2), ('57', 1), ('72', 1), ('73', 3), ('74', 1), ('78', 1), ('79', 2), ('80', 1), ('82', 4), ('84', 4), ('87', 1), ('89', 4), ('90', 1), ('94', 1), ('99', 2), ('102', 1), ('104', 1), ('105', 1), ('116', 1), ('120', 1), ('123', 1), ('126', 4), ('128', 1), ('129', 4), ('131', 2), ('133', 1), ('135', 1), ('139', 1), ('165', 1), ('170', 1), ('173', 1), ('174', 3), ('175', 1), ('176', 1), ('177', 2), ('179', 2), ('182', 1), ('184', 1), ('185', 1), ('187', 2), ('188', 2), ('189', 4), ('193', 3), ('195', 1), ('197', 1), ('198', 5), ('202', 2), ('205', 1), ('207', 2), ('212', 1), ('213', 1), ('214', 1), ('215', 1), ('216', 2), ('219', 1), ('222', 3), ('225', 1), ('228', 1), ('232', 3), ('233', 1), ('243', 4), ('245', 1), ('246', 1), ('251', 1), ('252', 2), ('256', 1), ('260', 1), ('261', 1), ('265', 1), ('272', 1), ('274', 1), ('280', 1), ('284', 1), ('287', 1), ('288', 2), ('293', 1), ('294', 2), ('312', 1), ('314', 1), ('317', 1), ('324', 1), ('330', 1), ('334', 1), ('339', 1), ('342', 1), ('344', 1), ('364', 1), ('365', 1), ('370', 1), ('372', 4), ('374', 3), ('381', 1), ('390', 1), ('393', 1), ('421', 1), ('423', 2), ('427', 2), ('432', 1), ('433', 1), ('434', 2), ('436', 1), ('442', 3), ('454', 1), ('462', 2), ('463', 4), ('464', 1), ('465', 1), ('466', 1), ('469', 1), ('470', 1), ('474', 1), ('486', 1), ('490', 1), ('495', 1), ('497', 3), ('505', 1), ('513', 1), ('516', 1), ('519', 1), ('522', 3), ('527', 2), ('529', 1), ('535', 1), ('540', 2), ('542', 1), ('549', 2), ('552', 3), ('553', 1), ('554', 1), ('565', 1), ('566', 2), ('567', 2), ('569', 2), ('571', 1), ('610', 1), ('618', 2), ('623', 1), ('625', 4), ('627', 2), ('634', 1), ('635', 1), ('636', 2), ('638', 1), ('643', 3), ('651', 1), ('655', 1), ('658', 2), ('662', 3), ('665', 1), ('673', 5), ('679', 1), ('689', 3), ('692', 4), ('693', 4), ('694', 1), ('695', 2), ('696', 2), ('699', 1), ('700', 1), ('709', 1), ('710', 2), ('711', 3), ('712', 6), ('713', 3), ('730', 1), ('739', 1), ('741', 1), ('743', 1), ('747', 2), ('757', 2), ('759', 2), ('763', 1), ('766', 2), ('772', 1), ('773', 1), ('780', 2), ('782', 4), ('796', 2), ('797', 1), ('799', 1), ('801', 2), ('804', 2), ('808', 3), ('809', 3), ('811', 3), ('812', 2), ('815', 2), ('816', 2), ('818', 1), ('825', 1), ('826', 1), ('827', 1), ('828', 1), ('830', 1), ('836', 2), ('838', 1), ('841', 2), ('844', 1), ('845', 1), ('850', 1), ('855', 1), ('857', 1), ('858', 2), ('859', 1), ('863', 1), ('875', 2), ('878', 2), ('887', 2), ('892', 1), ('893', 2), ('894', 1), ('902', 2), ('905', 2), ('907', 2), ('912', 1), ('926', 1), ('927', 3), ('932', 1), ('933', 2), ('946', 4), ('952', 1), ('953', 1), ('958', 1), ('959', 2), ('960', 2), ('970', 1), ('971', 2), ('972', 1), ('975', 1), ('986', 4), ('987', 1), ('991', 1), ('992', 2), ('993', 2), ('994', 4), ('996', 2), ('997', 1), ('999', 1), ('1001', 2), ('1002', 3), ('1004', 1), ('1019', 1), ('1039', 2), ('1047', 1), ('1056', 1), ('1062', 2), ('1063', 1), ('1065', 2), ('1066', 4), ('1068', 1), ('1072', 1), ('1074', 2), ('1075', 2), ('1078', 1), ('1081', 1), ('1082', 2), ('1083', 2), ('1091', 2), ('1092', 1), ('1093', 1), ('1094', 3), ('1095', 3), ('1097', 1), ('1098', 2), ('1100', 2), ('1104', 1), ('1114', 1), ('1116', 1), ('1119', 1), ('1130', 2), ('1144', 2), ('1145', 1), ('1146', 1), ('1153', 1), ('1155', 3), ('1156', 4), ('1159', 2), ('1161', 1), ('1162', 4), ('1163', 4), ('1164', 2), ('1165', 2), ('1166', 3), ('1171', 1), ('1172', 1), ('1173', 2), ('1177', 1), ('1190', 1), ('1191', 1), ('1192', 2), ('1194', 2), ('1199', 3), ('1205', 1), ('1211', 1), ('1213', 1), ('1220', 1), ('1222', 1), ('1225', 2), ('1227', 1), ('1230', 1), ('1231', 1), ('1239', 1), ('1243', 1), ('1247', 2), ('1249', 1), ('1252', 1), ('1253', 1), ('1257', 1), ('1271', 1), ('1272', 1), ('1274', 3), ('1277', 1), ('1290', 1), ('1303', 1), ('1305', 3), ('1309', 1), ('1313', 4), ('1317', 1), ('1319', 3), ('1320', 2), ('1323', 3), ('1326', 1), ('1333', 1), ('1335', 1), ('1337', 1), ('1338', 3), ('1341', 3), ('1342', 1), ('1343', 1), ('1349', 1), ('1350', 1), ('1352', 3), ('1353', 3), ('1354', 3), ('1364', 4), ('1367', 3), ('1371', 1), ('1373', 1), ('1377', 2), ('1381', 2), ('1383', 1), ('1387', 1), ('1393', 1), ('1395', 1), ('1400', 1)], 'aerodynam': [('1', 2), ('5', 1), ('11', 1), ('13', 1), ('14', 6), ('29', 3), ('32', 1), ('36', 1), ('44', 3), ('51', 5), ('52', 2), ('66', 5), ('73', 1), ('77', 2), ('95', 1), ('120', 1), ('129', 1), ('137', 3), ('141', 2), ('142', 2), ('163', 4), ('164', 1), ('172', 4), ('185', 2), ('202', 4), ('203', 1), ('204', 2), ('205', 2), ('216', 1), ('225', 2), ('237', 1), ('244', 1), ('272', 2), ('277', 1), ('284', 1), ('287', 3), ('289', 2), ('296', 2), ('297', 1), ('329', 4), ('337', 1), ('342', 1), ('360', 1), ('379', 1), ('390', 2), ('391', 3), ('406', 1), ('415', 3), ('434', 1), ('441', 6), ('442', 1), ('452', 1), ('453', 3), ('464', 1), ('481', 1), ('486', 1), ('499', 2), ('530', 2), ('536', 1), ('544', 1), ('546', 2), ('567', 3), ('592', 1), ('598', 1), ('599', 2), ('606', 4), ('608', 2), ('625', 1), ('627', 1), ('632', 1), ('634', 2), ('635', 1), ('638', 4), ('650', 1), ('658', 5), ('671', 1), ('685', 3), ('688', 4), ('689', 3), ('698', 1), ('704', 2), ('707', 1), ('708', 2), ('709', 2), ('711', 3), ('712', 3), ('715', 5), ('716', 1), ('717', 3), ('719', 2), ('746', 1), ('748', 3), ('749', 6), ('753', 6), ('759', 3), ('780', 1), ('781', 2), ('783', 2), ('792', 1), ('794', 1), ('798', 1), ('801', 2), ('812', 2), ('813', 2), ('814', 1), ('815', 4), ('859', 4), ('860', 4), ('861', 2), ('877', 3), ('886', 1), ('892', 1), ('894', 1), ('896', 1), ('899', 3), ('902', 1), ('917', 1), ('919', 1), ('925', 1), ('927', 1), ('933', 1), ('939', 1), ('947', 2), ('972', 2), ('978', 1), ('981', 1), ('982', 1), ('999', 4), ('1005', 1), ('1008', 1), ('1064', 1), ('1066', 10), ('1089', 2), ('1104', 3), ('1112', 3), ('1115', 3), ('1147', 1), ('1156', 1), ('1162', 1), ('1163', 1), ('1164', 3), ('1195', 1), ('1197', 4), ('1206', 1), ('1209', 2), ('1244', 3), ('1246', 1), ('1259', 1), ('1271', 2), ('1272', 3), ('1274', 1), ('1289', 2), ('1291', 2), ('1305', 1), ('1314', 1), ('1319', 1), ('1320', 2), ('1328', 2), ('1331', 1), ('1332', 2), ('1333', 5), ('1334', 3), ('1335', 4), ('1336', 2), ('1339', 2), ('1340', 1), ('1342', 3), ('1343', 1), ('1345', 3), ('1347', 2), ('1352', 3), ('1379', 2), ('1380', 5), ('1391', 1)], 'wing': [('1', 4), ('13', 4), ('14', 4), ('30', 3), ('31', 3), ('42', 6), ('52', 7), ('60', 1), ('69', 1), ('76', 3), ('78', 2), ('95', 3), ('97', 1), ('146', 4), ('147', 7), ('189', 5), ('191', 1), ('195', 4), ('199', 2), ('200', 6), ('202', 2), ('204', 5), ('205', 9), ('222', 6), ('224', 1), ('225', 11), ('226', 2), ('227', 5), ('229', 9), ('230', 7), ('244', 1), ('246', 4), ('247', 8), ('250', 5), ('251', 3), ('252', 1), ('256', 1), ('265', 1), ('279', 5), ('284', 4), ('287', 4), ('288', 7), ('289', 6), ('290', 1), ('311', 2), ('313', 1), ('315', 3), ('316', 1), ('333', 3), ('336', 2), ('360', 1), ('362', 1), ('364', 1), ('379', 1), ('395', 1), ('415', 2), ('416', 1), ('420', 9), ('431', 1), ('432', 5), ('433', 16), ('434', 5), ('441', 1), ('442', 5), ('453', 4), ('464', 9), ('465', 4), ('466', 1), ('486', 1), ('497', 4), ('503', 1), ('512', 1), ('513', 4), ('520', 5), ('545', 1), ('547', 3), ('561', 4), ('599', 2), ('600', 4), ('601', 5), ('612', 4), ('632', 4), ('633', 3), ('636', 2), ('637', 4), ('638', 7), ('643', 4), ('652', 2), ('671', 3), ('672', 1), ('673', 12), ('674', 4), ('675', 5), ('676', 4), ('677', 5), ('678', 6), ('679', 3), ('680', 5), ('681', 4), ('682', 3), ('683', 8), ('686', 2), ('692', 2), ('693', 3), ('694', 4), ('695', 5), ('696', 13), ('698', 7), ('699', 11), ('700', 3), ('703', 2), ('704', 3), ('705', 3), ('708', 3), ('709', 2), ('711', 1), ('712', 11), ('714', 1), ('747', 2), ('748', 4), ('749', 7), ('752', 4), ('755', 3), ('757', 3), ('768', 3), ('779', 9), ('780', 4), ('781', 2), ('783', 4), ('791', 4), ('792', 4), ('793', 5), ('794', 9), ('795', 1), ('796', 2), ('797', 9), ('798', 3), ('803', 3), ('808', 1), ('811', 5), ('860', 4), ('877', 5), ('879', 1), ('883', 1), ('895', 3), ('901', 3), ('902', 7), ('916', 5), ('917', 4), ('918', 3), ('919', 6), ('920', 3), ('921', 6), ('923', 3), ('924', 6), ('929', 2), ('970', 7), ('1062', 2), ('1064', 4), ('1074', 3), ('1075', 7), ('1089', 2), ('1091', 4), ('1092', 9), ('1094', 5), ('1095', 4), ('1108', 1), ('1111', 1), ('1115', 4), ('1128', 1), ('1144', 1), ('1162', 2), ('1163', 2), ('1164', 4), ('1168', 1), ('1169', 2), ('1170', 1), ('1184', 1), ('1186', 4), ('1188', 2), ('1197', 3), ('1202', 2), ('1207', 2), ('1208', 2), ('1218', 3), ('1220', 1), ('1229', 6), ('1233', 1), ('1239', 11), ('1243', 2), ('1246', 6), ('1266', 3), ('1272', 1), ('1276', 2), ('1277', 3), ('1280', 6), ('1289', 7), ('1290', 4), ('1294', 1), ('1320', 1), ('1328', 1), ('1331', 2), ('1332', 5), ('1333', 7), ('1334', 6), ('1336', 2), ('1337', 6), ('1338', 3), ('1339', 5), ('1340', 7), ('1341', 10), ('1342', 6), ('1343', 7), ('1355', 2), ('1362', 4), ('1380', 1)], 'slipstream': [('1', 6), ('409', 1), ('453', 6), ('484', 7), ('1064', 5), ('1090', 1), ('1091', 1), ('1094', 4), ('1095', 2), ('1144', 10), ('1164', 1), ('1165', 1), ('1166', 1)]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)  # {term: [(doc_id, freq), ...]}\n",
    "    \n",
    "    for doc_id, title, author, bib, text in documents:\n",
    "        if not text:  # Skip documents without text\n",
    "            continue\n",
    "        else:\n",
    "            if not author:\n",
    "                author = \"\"\n",
    "            if not bib:\n",
    "                bib = \"\"\n",
    "            processed_text = preprocess_text(title + \" \" + text )  # Combine title & text\n",
    "            term_freq = defaultdict(int)\n",
    "            \n",
    "            # Count term frequency in the document\n",
    "            for term in processed_text:\n",
    "                term_freq[term] += 1\n",
    "            \n",
    "            # Add term and frequency to the inverted index\n",
    "            for term, freq in term_freq.items():\n",
    "                inverted_index[term].append((doc_id, freq))\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:5]))  # Display first 5 terms in the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_tf(freq, doc_length, max_freq = 0, method=\"augmented\"):\n",
    "    if method == \"raw\":\n",
    "        return freq / doc_length\n",
    "    elif method == \"log\":\n",
    "        return 1 + math.log(freq) if freq > 0 else 0\n",
    "    elif method == \"augmented\":\n",
    "        return 0.5 + (0.5 * freq / max_freq)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'experimental investigation of the aerodynamics of a\\nwing in a slipstream .', 'brenckman,m.', 'j. ae. scs. 25, 1958, 324.', 'experimental investigation of the aerodynamics of a\\nwing in a slipstream .\\n  an experimental study of a wing in a propeller slipstream was\\nmade in order to determine the spanwise distribution of the lift\\nincrease due to slipstream at different angles of attack of the wing\\nand at different free stream to slipstream velocity ratios .  the\\nresults were intended in part as an evaluation basis for different\\ntheoretical treatments of this problem .\\n  the comparative span loading curves, together with\\nsupporting evidence, showed that a substantial part of the lift increment\\nproduced by the slipstream was due to a /destalling/ or\\nboundary-layer-control effect .  the integrated remaining lift\\nincrement, after subtracting this destalling lift, was found to agree\\nwell with a potential flow theory .\\n  an empirical evaluation of the destalling effects was made for\\nthe specific configuration of the experiment .'), ('2', 'simple shear flow past a flat plate in an incompressible fluid of small\\nviscosity .', 'ting-yili', 'department of aeronautical engineering, rensselaer polytechnic\\ninstitute\\ntroy, n.y.', \"simple shear flow past a flat plate in an incompressible fluid of small\\nviscosity .\\nin the study of high-speed viscous flow past a two-dimensional body it\\nis usually necessary to consider a curved shock wave emitting from the\\nnose or leading edge of the body .  consequently, there exists an\\ninviscid rotational flow region between the shock wave and the boundary\\nlayer .  such a situation arises, for instance, in the study of the\\nhypersonic viscous flow past a flat plate .  the situation is somewhat\\ndifferent from prandtl's classical boundary-layer problem . in prandtl's\\noriginal problem the inviscid free stream outside the boundary layer is\\nirrotational while in a hypersonic boundary-layer problem the inviscid\\nfree stream must be considered as rotational .  the possible effects of\\nvorticity have been recently discussed by ferri and libby .  in the\\npresent paper, the simple shear flow past a flat plate in a fluid of small\\nviscosity is investigated .  it can be shown that this problem can again\\nbe treated by the boundary-layer approximation, the only novel feature\\nbeing that the free stream has a constant vorticity .  the discussion\\nhere is restricted to two-dimensional incompressible steady flow .\")]\n"
     ]
    }
   ],
   "source": [
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute TF-IDF matric for VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample doc length: 77\n",
      "Summed TF from index: 0\n",
      "{'1': {'experiment': 5.737841054382719, 'investig': 4.498056615657707, 'aerodynam': 5.88820625210848, 'wing': 7.718907909927602, 'slipstream': 17.67374383820254, 'studi': 3.1256046045635446, 'propel': 5.50807897937778, 'made': 4.546207163792112, 'order': 3.6442894977632214, 'determin': 2.868829620446816, 'spanwis': 5.547717444047467, 'distribut': 2.6628827758605262, 'lift': 8.843078186426647, 'increas': 3.287975856727465, 'due': 6.261100412521749, 'differ': 7.078507506621823, 'angl': 3.3212012029332643, 'attack': 4.022866775442388, 'free': 4.043981041249094, 'stream': 3.622067528674192, 'veloc': 2.8281529384874706, 'ratio': 2.9110259821991606, 'result': 1.9230912840291214, 'intend': 6.414393773893241, 'part': 6.939466588013481, 'evalu': 7.613525437098513, 'basi': 4.528946690092039, 'theoret': 3.1398731774225555, 'treatment': 5.08971203276697, 'problem': 2.7818959420778144, 'compar': 3.083849448202598, 'span': 5.4697844533932525, 'load': 3.3554335296663074, 'curv': 3.9519281125942523, 'togeth': 5.116932153071074, 'support': 4.562172036297838, 'evid': 5.29565887735326, 'show': 3.3155953337023814, 'substanti': 5.08971203276697, 'increment': 10.238127503154494, 'produc': 4.405032950758269, 'destal': 16.937352644948966, 'boundarylayercontrol': 8.528744697620185, 'effect': 3.7367269281685958, 'integr': 3.690089568515421, 'remain': 4.870397217587137, 'subtract': 8.07073928633969, 'found': 2.7853876924057928, 'agre': 4.6868190313091755, 'well': 3.651795042530893, 'potenti': 4.744758927793769, 'flow': 1.8877890255735292, 'theori': 2.403318664398648, 'empir': 5.588797619610542, 'specif': 4.6497802923707505, 'configur': 4.333966730986997, 'experi': 3.6294263842225645}, '11': {'experiment': 3.2132481449378094, 'aerodynam': 4.087109429823536, 'studi': 3.6733473886181787, 'differ': 6.711694197170214, 'free': 8.046958955742772, 'stream': 7.207409837205157, 'veloc': 3.3237691664638613, 'theoret': 3.6901164402040476, 'problem': 6.861215438006099, 'effect': 2.5937290935290482, 'situat': 6.7810597144584275, 'pressur': 5.46919629427205, 'gradient': 7.943359627840646, 'solut': 5.0174356544438945, 'laminar': 6.286333532948802, 'one': 3.707100027049598, 'may': 3.707100027049598, 'occur': 4.544450147436658, 'gener': 3.2450468452559194, 'superson': 3.5114436682009056, 'perform': 5.093467468785042, 'institut': 8.026642933329766, 'two': 3.3195074219839, 'similar': 7.31197322581786, 'compress': 6.676845320621943, 'mix': 15.219114204559945, 'mani': 5.981652438406807, 'practic': 4.816808059272569, 'interest': 5.4018956650245835, 'wherein': 13.855029665989589, 'stagnat': 4.566036513546053, 'anoth': 6.6703679382514025, 'major': 6.301802850019919, 'interact': 4.9997203698049155, 'take': 5.599745205051758, 'place': 6.262172078221109, 'presenc': 9.011973042450492, 'axial': 7.470327953387292, 'characterist': 4.013338659432403, 'influenc': 4.803728817856248, 'significantli': 7.034691705733343, 'devic': 7.886773449295491, 'phenomena': 6.301802850019919, 'cite': 8.806948696155446, 'program': 6.724560453114763, 'research': 5.57624844536848, 'carri': 5.21192467788654, 'polytechn': 10.023354207199535, 'brooklyn': 10.023354207199535}, '12': {'experiment': 2.924924769324915, 'attack': 4.3036213035062065, 'problem': 2.9760435303371393, 'load': 3.5896081145224956, 'well': 3.906652598344136, 'highspe': 11.308804476757366, 'boundari': 2.679834275856947, 'layer': 2.852085849549698, 'origin': 5.258643593055713, 'discuss': 5.396813711505507, 'present': 2.434955313428352, 'heat': 3.1438153459225298, 'subject': 4.4087670144142, 'input': 7.3064157112202786, 'analyt': 4.314862439130813, 'one': 3.3744635342168046, 'thermal': 7.729061444482464, 'method': 2.568380831794509, 'speed': 5.585811527406611, 'upon': 4.744260915616639, 'avail': 4.636432820997829, 'suggest': 4.760481472791865, 'anoth': 6.071838688785706, 'research': 5.075894067958294, 'structur': 12.014675526035036, 'aerelast': 15.448212127878516, 'consider': 6.482490618689668, 'high': 6.174415285398354, 'flight': 9.96588650517734, 'domin': 7.1790966558995555, 'factor': 7.900684779493168, 'design': 3.9728836038661672, 'aircraft': 8.173892904138608, 'aeroelast': 10.842020293599793, 'matter': 8.286354623013406, 'concern': 4.898736470310763, 'larg': 3.5529867251450886, 'interrel': 8.286354623013406, 'summari': 6.611137988468399, 'tool': 7.063922560752333, 'aeronaut': 6.53787838518483, 'engin': 5.631199076675272, 'meet': 7.610107425307063, 'demand': 8.286354623013406, 'state': 5.034220511867653, 'art': 8.633993484389345, 'respect': 4.4973815049225765, 'transfer': 3.6663585628880897, 'mode': 5.075894067958294, 'failur': 5.700270067912325, 'combin': 4.259674156217049, 'acrothermoelast': 9.123962940285908, 'allevi': 7.448746305740902, 'summar': 6.53787838518483, 'final': 4.974286905305113, 'avenu': 8.633993484389345, 'fundament': 5.811895222577617}, '16': {'experiment': 2.7598323863985574, 'stream': 3.6561410978875655, 'result': 1.941182217853866, 'show': 3.3467858529823458, 'remain': 4.9162140929977465, 'flow': 1.9055478634510414, 'shear': 4.148462231679279, 'incompress': 6.5323607837849496, 'viscos': 4.730908947189988, 'boundari': 6.033925713192635, 'layer': 7.022270721953438, 'boundarylay': 3.6054569076208987, 'shown': 2.950308545772431, 'equat': 2.5621135114199047, 'pressur': 2.238358991126359, 'laminar': 3.188901825202066, 'friction': 4.571551466863005, 'analyt': 4.071317398401456, 'temperatur': 3.0677865632684282, 'gener': 2.787144028395942, 'point': 3.234014781199479, 'given': 2.6054725964577483, 'element': 5.684404410978817, 'turbul': 8.983955344623066, 'measur': 3.090100514879638, 'function': 3.428682025068921, 'relat': 3.678572113825064, 'valu': 2.8732382333623874, 'good': 3.7895129681800745, 'agreement': 3.465664938513692, 'caus': 4.5389719208167225, 'mean': 3.724803035705355, 'coeffici': 3.228913177444594, 'analysi': 2.986765856932057, 'compress': 7.107997335774981, 'modif': 5.3133556627006975, 'applic': 3.249457995206622, 'compon': 4.712062129052342, 'comput': 3.903190830446159, 'essenti': 4.712062129052342, 'demonstr': 5.137592049870931, 'includ': 3.223834298057919, 'thu': 4.431759497755798, 'transform': 13.772949154543065, 'equival': 5.058925926791648, 'first': 3.6341428617808087, 'stewartson': 7.356331522336057, 'except': 4.65732170349738, 'explicit': 5.983553860413008, 'requir': 3.716966533813092, 'key': 7.8186454891467365, 'fluctuat': 5.729124124503958, 'postul': 7.180567909506291, 'appar': 5.41256660354201, 'associ': 4.571551466863005, 'mass': 4.6222356571296705, 'invari': 7.180567909506291, 'rise': 5.137592049870931, 'separ': 3.678572113825064, 'independ': 4.851041695890019, 'report': 4.103742518154139, 'selfpreserv': 8.146662468748396}, '17': {'experiment': 2.7088666034734494, 'studi': 3.0967443580915153, 'show': 3.2849808092700683, 'found': 2.759668836218539, 'flow': 4.880583440049351, 'experi': 3.5959141031593056, 'incompress': 6.411727776021268, 'viscos': 9.744997081109773, 'bodi': 2.931609297039966, 'necessari': 4.643543323237883, 'boundari': 4.202188869040647, 'layer': 4.47229275276493, 'hyperson': 3.4779998441119866, 'boundarylay': 3.53887498945614, 'equat': 2.514798999967666, 'comparison': 3.687324100987802, 'case': 2.619796293661243, 'gener': 2.735673881791276, 'refer': 4.681484257006419, 'space': 5.3126128179631005, 'turbul': 6.25666919740018, 'avail': 4.293949081961514, 'data': 5.41751380680658, 'suggest': 4.408834515451938, 'extend': 4.1651692000870755, 'desir': 5.496492636994579, 'extens': 4.349918262160452, 'compress': 8.674948893772909, 'mix': 8.324927754898575, 'carri': 4.393812137412505, 'differenti': 4.0066409728961565, 'applic': 5.400208789027944, 'estim': 6.783832866775083, 'sever': 3.7611409843988026, 'form': 3.154395748725553, 'transform': 10.950561152591114, 'mass': 4.5368769011809515, 'invari': 7.047964470558877, 'remark': 10.619806091654649, 'eddi': 13.729384247488888, 'connect': 5.3126128179631005, 'wake': 8.365679568041335, 'behind': 4.48712869508912, 'missil': 5.347050930843366, 'vehicl': 4.571315014061217, 'divis': 7.674258681189812, 'electr': 5.042716215214474, 'compani': 8.44999459797196, 'axisymmetr': 4.720756720212431, 'lack': 6.542125156525353, 'make': 4.825426441721558, 'ration': 6.898522764407663, 'appli': 3.2684819793539863, 'infinitesim': 6.444746342627559, 'mager': 7.674258681189812, 'partial': 5.0696849987436625, 'valid': 4.455150795026767, 'assumpt': 3.9754068426227125, 'establish': 4.870198426363643}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_tf_idf(inverted_index, documents):\n",
    "    \"\"\"Compute the TF-IDF matrix from an inverted index.\"\"\"\n",
    "    N = len(documents)  # Total number of documents\n",
    "    tf_idf = defaultdict(dict)  # Store TF-IDF scores\n",
    "\n",
    "    # Compute document lengths based on preprocessed text\n",
    "    doc_lengths = {doc[0]: len(preprocess_text(doc[4])) for doc in documents}\n",
    "    avg_doc_length = sum(doc_lengths.values()) / N  # Compute average document length\n",
    "\n",
    "    print(\"Sample doc length:\", doc_lengths['1'])  # Verify document length\n",
    "    print(\"Summed TF from index:\", sum(inverted_index[term]['1'] for term in inverted_index if '1' in inverted_index[term]))\n",
    "\n",
    "    for term, doc_freqs in inverted_index.items():\n",
    "        df = len(doc_freqs)  # Number of documents containing the term\n",
    "        if df == 0:\n",
    "            print(f\"Warning: Term '{term}' has df=0, check indexing!\")\n",
    "\n",
    "        idf = math.log((N + 1) / (df + 1)) + 1  # Smoothed IDF to avoid zero division\n",
    "\n",
    "        for (doc_id, freq) in doc_freqs:\n",
    "            doc_length = doc_lengths[doc_id]  # Get the length of the document\n",
    "            tf = (1 + math.log(freq)) if freq > 0 else 0  # Logarithmic TF\n",
    "            \n",
    "            # BM25-like length normalization\n",
    "            tf /= (1 - 0.75 + 0.75 * (doc_length / avg_doc_length))\n",
    "\n",
    "            tf_idf_score = tf * idf  # Compute final TF-IDF score\n",
    "\n",
    "            tf_idf[doc_id][term] = tf_idf_score  # Store the score\n",
    "\n",
    "    return tf_idf\n",
    "\n",
    "tf_idf = compute_tf_idf(inverted_index, documents)\n",
    "print(dict(list(tf_idf.items())[:5]))  # Display first 5 documents in the TF-IDF matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experiment', 'investig', 'aerodynam', 'wing', 'slipstream']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_text(\"experimental investigation of the aerodynamics of a wing in a slipstream.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup dictionary for document titles and full texts\n",
    "doc_lookup = {doc[0]: (doc[1], doc[4]) for doc in documents}  # { \"1\": (\"Title 1\", \"Full text...\"), ... }\n",
    "\n",
    "def display_results(ranked_results, doc_lookup, top_n=10):\n",
    "    \"\"\"Display the top N ranked results with their titles and allow the user to read one.\"\"\"\n",
    "    print(\"\\nTop Search Results:\\n\")\n",
    "    for rank, (doc_id, score) in enumerate(ranked_results[:top_n], start=1):\n",
    "        title = doc_lookup.get(doc_id, (\"Unknown Title\", \"\"))[0]\n",
    "        print(f\"{rank}. [{doc_id}] {title} (Score: {score:.4f})\")\n",
    "\n",
    "    # Ask user if they want to read a document\n",
    "    doc_id_to_read = input(\"\\nEnter a document ID to read (or press Enter to skip): \").strip()\n",
    "    if doc_id_to_read in doc_lookup:\n",
    "        title, text = doc_lookup[doc_id_to_read]\n",
    "        print(f\"\\n=== {title} ===\\n{text[:500]}...\")  # Show only first 500 characters\n",
    "    else:\n",
    "        print(\"No document selected or invalid ID.\")\n",
    "\n",
    "# display_results(ranked_results, doc_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_queries(filename):\n",
    "    \"\"\"Parse queries from the Cranfield XML file.\"\"\"\n",
    "    queries = {}\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for top in root.findall(\"top\"):\n",
    "        num = top.find(\"num\").text.strip()  # Extract query number\n",
    "        title = top.find(\"title\").text.strip()  # Extract query text\n",
    "        \n",
    "        if num and title:\n",
    "            queries[num] = title\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(scores):\n",
    "    \"\"\"Normalize scores using Min-Max scaling, keeping document IDs as strings.\"\"\"\n",
    "    scores = {str(doc_id): float(score) for doc_id, score in scores.items()}  # Ensure float values\n",
    "    min_score = min(scores.values())\n",
    "    max_score = max(scores.values())\n",
    "    if max_score - min_score == 0:\n",
    "        return {doc_id: 0.0 for doc_id in scores}  # Avoid division by zero\n",
    "    return {doc_id: (score - min_score) / (max_score - min_score) for doc_id, score in scores.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_cosine_similarity(tf_idf, query, inverted_index, N):\n",
    "    \"\"\"Compute cosine similarity between query and documents.\"\"\"\n",
    "    query_tf_idf = {}\n",
    "\n",
    "    # Compute TF-IDF for query using the same IDF values as the document matrix\n",
    "    for term in query:\n",
    "        if term in inverted_index:\n",
    "            df = len(inverted_index[term])  # Document frequency\n",
    "            idf = math.log(N / df) if df > 0 else 0  # Compute IDF\n",
    "            tf = compute_tf(query.count(term), len(query), method=\"raw\")  # Query term frequency\n",
    "            query_tf_idf[term] = tf * idf  # TF-IDF for query\n",
    "\n",
    "    # Compute cosine similarity for each document\n",
    "    scores = {}\n",
    "    for doc_id, doc_vector in tf_idf.items():\n",
    "        doc_norm = np.linalg.norm(list(doc_vector.values()))  # Document vector norm\n",
    "        query_norm = np.linalg.norm(list(query_tf_idf.values()))  # Query vector norm\n",
    "\n",
    "        # Compute dot product\n",
    "        dot_product = sum(doc_vector.get(term, 0) * query_tf_idf.get(term, 0) for term in query_tf_idf)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        if doc_norm > 0 and query_norm > 0:\n",
    "            scores[doc_id] = dot_product / (doc_norm * query_norm)\n",
    "        else:\n",
    "            scores[doc_id] = 0\n",
    "\n",
    "    # Normalize scores\n",
    "    if type(scores) == str:\n",
    "        print(scores)\n",
    "    # scores = min_max_normalize(scores)\n",
    "    \n",
    "    # Rank documents by score\n",
    "    ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(queries): 225\n"
     ]
    }
   ],
   "source": [
    "vsm_results = {}\n",
    "\n",
    "queries = parse_queries(\"cran_updated.qry.xml\")  # Load queries from XML file\n",
    "print(\"len(queries):\", len(queries))\n",
    "# print(queries)\n",
    "\n",
    "for query_id, query_text in queries.items():\n",
    "    query_text = preprocess_text(query_text)  # Apply the same preprocessing as indexing\n",
    "    ranked_docs = compute_cosine_similarity(tf_idf, query_text, inverted_index, len(documents))\n",
    "    vsm_results[query_id] = ranked_docs\n",
    "    # if query_id == '1':\n",
    "    #     print(\"query = \", query_text)\n",
    "    #     display_results(ranked_docs, doc_lookup, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_trec_format(ranked_results, model_name, output_file):\n",
    "    \"\"\"\n",
    "    Save ranked results in TREC format: query_id Q0 doc_id rank score model_name\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for query_id, results in ranked_results.items():\n",
    "            for rank, (doc_id, score) in enumerate(results, start=1):\n",
    "                f.write(f\"{query_id} 0 {doc_id} {rank} {score:.4f} {model_name}\\n\")\n",
    "\n",
    "# Save VSM results\n",
    "save_results_trec_format(vsm_results, \"VSM\", \"vsm_results_just_text.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_bm25_2(inverted_index, documents, queries, k1=1.5, b=0.75):\n",
    "    N = len(documents)  # Total number of documents\n",
    "    doc_lengths = {doc[0]: len(doc[4].split()) for doc in documents}  # Document lengths\n",
    "    avg_doc_length = sum(doc_lengths.values()) / N  # Average document length\n",
    "    \n",
    "    # Compute IDF for each term\n",
    "    idf = {}\n",
    "    for term, doc_freqs in inverted_index.items():\n",
    "        df = len(doc_freqs)  # Number of documents containing the term\n",
    "        idf[term] = math.log((N - df + 0.5) / (df + 0.5) + 1)\n",
    "    \n",
    "    bm25_scores = defaultdict(dict)  # Store BM25 scores\n",
    "    \n",
    "    for query_id, query_text in queries.items():\n",
    "        query_terms = preprocess_text(query_text)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            if term in inverted_index:\n",
    "                for doc_id, tf in inverted_index[term]:\n",
    "                    doc_length = doc_lengths[doc_id]\n",
    "                    tf_weight = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avg_doc_length)))        # compute tf weights\n",
    "                    bm25_scores[query_id][doc_id] = bm25_scores[query_id].get(doc_id, 0) + idf[term] * tf_weight\n",
    "    \n",
    "    return bm25_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_bm25(inverted_index, documents, queries, k1=1.5, b=0.75):\n",
    "    \"\"\"\n",
    "    Compute BM25 scores for documents given a query.\n",
    "    \n",
    "    Args:\n",
    "        inverted_index (dict): Inverted index mapping terms to (doc_id, tf) pairs.\n",
    "        documents (list): List of documents, where each document is a tuple (doc_id, title, author, bib, text).\n",
    "        queries (dict): Dictionary of queries, where keys are query IDs and values are query texts.\n",
    "        k1 (float): Controls term frequency saturation. Default is 1.5.\n",
    "        b (float): Controls document length normalization. Default is 0.75.\n",
    "    \n",
    "    Returns:\n",
    "        dict: BM25 scores for each query, mapped to document IDs.\n",
    "    \"\"\"\n",
    "    N = len(documents)  # Total number of documents\n",
    "    doc_lengths = {doc[0]: len(doc[4].split()) for doc in documents}  # Document lengths\n",
    "    avg_doc_length = sum(doc_lengths.values()) / N  # Average document length\n",
    "    \n",
    "    # Compute IDF for each term using Robertson-Sparck Jones formula\n",
    "    idf = {}\n",
    "    for term, doc_freqs in inverted_index.items():\n",
    "        df = len(doc_freqs)  # Number of documents containing the term\n",
    "        idf[term] = math.log((N - df + 0.5) / (df + 0.5) + 1)\n",
    "    \n",
    "    bm25_scores = defaultdict(dict)  # Store BM25 scores\n",
    "    \n",
    "    for query_id, query_text in queries.items():\n",
    "        query_terms = preprocess_text(query_text)  # Preprocess query text\n",
    "        \n",
    "        for term in query_terms:\n",
    "            if term in inverted_index:\n",
    "                for doc_id, tf in inverted_index[term]:\n",
    "                    doc_length = doc_lengths[doc_id]\n",
    "                    \n",
    "                    # Compute TF weight with saturation and document length normalization\n",
    "                    tf_weight = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avg_doc_length)))\n",
    "                    \n",
    "                    # Compute BM25 score contribution for this term\n",
    "                    bm25_scores[query_id][doc_id] = bm25_scores[query_id].get(doc_id, 0) + idf[term] * tf_weight\n",
    "    \n",
    "    return bm25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 results saved.\n"
     ]
    }
   ],
   "source": [
    "bm25_scores = compute_bm25(inverted_index, documents, queries)\n",
    "\n",
    "# Format results in TREC format and save\n",
    "with open(\"bm25_results2.txt\", \"w\") as f:\n",
    "    for query_id, scores in bm25_scores.items():\n",
    "        ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:1000]  # Keep top 1000 results\n",
    "        for rank, (doc_id, score) in enumerate(ranked_docs, start=1):\n",
    "            f.write(f\"{query_id} 0 {doc_id} {rank} {score:.4f} BM25\\n\")\n",
    "\n",
    "print(\"BM25 results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N GRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESTART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    return [' '.join(text[i:i+n]) for i in range(len(text) - n + 1)]\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def build_ngram_index(documents, n):\n",
    "    \"\"\"Build an inverted index for n-grams with frequencies.\"\"\"\n",
    "    ngram_index = defaultdict(dict)  # Now stores {ngram: {doc_id: frequency}}\n",
    "    for doc_id, title, author, bib, text in documents:\n",
    "        if not text:  # Skip documents without text\n",
    "            continue\n",
    "        else:\n",
    "            if not author:\n",
    "                author = \"\"\n",
    "            if not bib:\n",
    "                bib = \"\"\n",
    "            tokens = preprocess_text(title + \" \" + text + \" \"  + author + \" \" + bib) # tokenises words and applies filters\n",
    "            ngrams = generate_ngrams(tokens, n)\n",
    "            ngram_freq = Counter(ngrams)  # Count frequency of each n-gram in the document\n",
    "            for ngram, freq in ngram_freq.items():\n",
    "                if doc_id in ngram_index[ngram]:\n",
    "                    ngram_index[ngram][doc_id] += freq\n",
    "                else:\n",
    "                    ngram_index[ngram][doc_id] = freq\n",
    "    print(f\"Number of n-grams in index: {len(ngram_index)} and first is {list(ngram_index.keys())[0]}\")\n",
    "    return ngram_index\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "def compute_ngram_similarity(ngram_index, query, documents, n):\n",
    "    \"\"\"Compute n-gram overlap similarity between query and documents with weighted scores.\"\"\"\n",
    "    try:\n",
    "        query = preprocess_text(query)\n",
    "        query_ngrams = generate_ngrams(query, n)\n",
    "        # print(f\"Query N-grams: {query_ngrams}\")\n",
    "\n",
    "        # Calculate IDF for each n-gram in the query\n",
    "        N = len(documents)  # Total number of documents\n",
    "        idf = {}\n",
    "        for ngram in query_ngrams:\n",
    "            if ngram in ngram_index:\n",
    "                df = len(ngram_index[ngram])  # Document frequency of the n-gram\n",
    "                idf[ngram] = math.log(N / df)\n",
    "            else:\n",
    "                idf[ngram] = 0  # If n-gram is not in the index, IDF is 0\n",
    "\n",
    "        scores = defaultdict(float)\n",
    "        for ngram in query_ngrams:\n",
    "            if ngram in ngram_index:\n",
    "                for doc_id, freq in ngram_index[ngram].items():\n",
    "                    # Weighted score: frequency of n-gram in document * IDF of n-gram\n",
    "                    scores[doc_id] += freq * idf[ngram]\n",
    "\n",
    "        # print(f\"Scores before normalization: {scores}\")\n",
    "\n",
    "        # Rank documents by score (limit to top 100)\n",
    "        if not scores:\n",
    "            return []  # Return an empty list if no scores\n",
    "        else:\n",
    "            ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "            return ranked_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {query}\\nquery_ngrams = {query_ngrams}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of n-grams in index: 8971 and first is experiment\n"
     ]
    }
   ],
   "source": [
    "Number_n = 1\n",
    "ngram_index = build_ngram_index(documents, n=Number_n)  # Change n as needed\n",
    "# ranked_results = compute_ngram_similarity(ngram_index, queries, documents, n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(ngram_index[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def compute_collection_probabilities(documents, n):\n",
    "    \"\"\"Compute the probability of each n-gram in the corpus.\"\"\"\n",
    "    ngram_counter = Counter()\n",
    "    total_ngrams = 0\n",
    "\n",
    "    for doc_id, title, author, bib, text in documents:\n",
    "        if not text:\n",
    "            continue\n",
    "        else :\n",
    "            if not author:\n",
    "                author = \"\"\n",
    "            if not bib:\n",
    "                bib = \"\"\n",
    "            if not title:\n",
    "                title = \"\"\n",
    "            tokens = preprocess_text(title + \" \" + text + \" \" + author + \" \" + bib)\n",
    "            ngrams = generate_ngrams(tokens, n)\n",
    "            ngram_counter.update(ngrams)\n",
    "            total_ngrams += len(ngrams)\n",
    "\n",
    "    collection_prob = {ngram: count / total_ngrams for ngram, count in ngram_counter.items()}\n",
    "    return collection_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "queries = parse_queries(\"cran_updated.qry.xml\")  # Load queries from XML file\n",
    "for query_id, query in queries.items():\n",
    "    ranked_results = compute_ngram_similarity(ngram_index, query, documents, n=Number_n)\n",
    "    all_results[query_id] = ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_trec_format2(ranked_results, model_name, output_file):\n",
    "    \"\"\"\n",
    "    Save ranked results in TREC format: query_id Q0 doc_id rank score model_name\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for query_id, results in ranked_results.items():\n",
    "            for rank, (doc_id, score) in enumerate(results[:100], start=1):\n",
    "                f.write(f\"{query_id} 0 {doc_id} {rank} {score:.4f} {model_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_trec_format2(all_results, \"ngram_weighted\", \"ngram_weighted_results2.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
